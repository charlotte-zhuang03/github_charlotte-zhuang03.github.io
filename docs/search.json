[
  {
    "objectID": "posts/Blog_Post_Perceptron/Blog_Post_Perceptron.html",
    "href": "posts/Blog_Post_Perceptron/Blog_Post_Perceptron.html",
    "title": "Blog Post: Perceptron",
    "section": "",
    "text": "Implementation of perceptron update\nPerceptron update of weight vector w follows this logic:\n\n\nCompute the dot product of w and X at index i. The sign of the dot product indicates the expected value of y.\n\nif dot product > 0, y is expected to be positive;\n\nif dot product < 0, y is expected to be negative;\nthe result can be computed using Perceptron.predict() function.\n\n\nLet actual_y * expected_y:\n\n\nif result >= 0, the expected value and actual value have the same sign, so the result is well-predicted, and there is no need to update w\n\nif result < 0, the expected value and actual value have different signs, so the result is not predicted well, therefore we need to update w to get a weight vector that can better predict the result\n\n\nAssign 0 to actual_y * expected_y >=0 and 1 to actual_y * expected_y < 0. The 0s and 1s can be considered as a determinant that helps to determine whether w will be updated:\n\n\nWhen determinant = 0, the value to update is 0, meaning no update is made;\n\nWhen determinant = 1, it means to update the w.\n\n\nFor every step in the updating process:\nWe add determinant * y_tilde (which is y * 2 - 1, and basically replace the 0 in y by -1) * the vector at index i of X_ to the original w to get an updated weight vector.\n\n\nThe function I used to update the weight vector is mainly Perceptron.fit(), while Perceptron.predict() is called inside Perceptron.fit() to help computing the determinant. Both Perceptron.predict() and the parts of Perceptron.fit() related to updating the weight vector w is below:\ndef predict2(self, X):\n    \"\"\"\n    Return a list of vector showing whether the dot product of w and X is greater than\n    -1: smaller than 0\n    1: greater than 0\n    \"\"\"\n\n    return np.sign(np.dot(X, self.w))\n\ndef fit(self, X, y, max_steps=1000):\n    \"\"\"\n    Update the weight vector w and the history vector which keeps track of the change of accuracy       \n    Parameters:\n    X: a matrix of predictor variables, with X.shape[0] observations and X.shape[1] features\n    y: a vector of binary labels 0 and 1\n    max_steps: number of loops going through, set default to 1000        \n    Return:\n    None\n    \"\"\"\n    \n    X_ = np.append(X, np.ones((X.shape[0], 1)), 1) # initialize X_ by appending 1 at the end of X            \n    \n    self.w = np.random.rand(X_.shape[1]) # initialize a random weight vector w \n    \n    for _ in range(max_steps):\n        i = np.random.randint(X_.shape[0]) # pick a random index i within the range of number of observations\n        y_tilde = 2*y-1 # transform y from a vector containing 0 and 1 to a vector containing -1 and 1\n        \n        determine = np.where((y_tilde[i]*self.predict2(X_[i])) < 0, 1, 0) \n        # determine = 0 if predicted and actual y have same sign\n        # determine = 1 if predicted and actual y have different sign, which triggers the update of w\n\n        self.w = self.w + determine*y_tilde[i]*X_[i] # update w\n\n\nExperiments\n\n1. 2D data in the example:\nVisualization of data:\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nfrom perceptron import Perceptron\n\np = Perceptron()\np.fit(X, y)\n\np.w\nprint(p.history[-10:])\n\n[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n\n\n\n\n\nEvolution of the score over training:\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nLine in the final iteration\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\n2. 2D data not linearly separable:\nVisualization of data:\n\nnp.random.seed(1234)\n\nn1 = 100\np1_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p1_features - 1, centers = [(-0.5, -0.5), (0.5, 0.5)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nfrom perceptron import Perceptron\n\np = Perceptron()\np.fit(X, y)\n\np.w\nprint(p.history[-10:])\n\n[0.48, 0.48, 0.48, 0.48, 0.48, 0.48, 0.48, 0.48, 0.42, 0.42]\n\n\n\n\n\nEvolution of the score over training:\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nLine in the final iteration:\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\n3. Data of more than two dimensions\n\nnp.random.seed(123)\n\nn2 = 100\np2_features = 5\n\nX, y = make_blobs(n_samples = 100, n_features = p2_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nfrom perceptron import Perceptron\n\np = Perceptron()\np.fit(X, y)\n\np.w\nprint(p.history[-10:])\n\n[0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96]\n\n\n\n\n\nEvolution of score over training period:\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nI believe that the data is not linearly separable. The max_steps I set was 1000, and after 1000 iterations, the accuracy stop increasing when it reaches the value of 0.96, indicating that it could draw a line that almost separate the data, but still not able to completely separate the data. Thus, the data is not linearly separable.\n\n\n\nRuntime Analysis\nThere is addition and multiplication involved in this operation.\nThe runtime complexity of a single iteration depends on the number of features p. This is because the weight vector w and the vector at index i of X which we compute dot product of, are all vector of p elements. If p increases, the runtime increases since more calculations need to be computed.\nThe runtime complexity of a single iteration does NOT depend on the number of data points n. Though y and predicted vector of y (denoted as y_predict) are vector of n elements, when we compute for updates, only the value at the index i of the y and y_predict will be used, so there is no looping through the whole list, thus n has no effect on the runtime.\nCalculating the dot product of w and X[i] should be O(n) since it loops through the whole list.\nMultiplying y_tilde[i] with the dot product would be O(1) since it is just the multiplication of two numbers.\nThe np.sign() operation here is also O(1) in 1 iteration since it only determines whether the result of previous two operations is larger or smaller than 0.\nMultiplying the result of previous three operations with y_tilde[i] and X[i] is has the complexity of O(n) since X[i] is a vector, and multiplication needs to loop through the whole vector.\nAddition of update vector to previous weight vector w has time complexity of O(n), since it needs to loop through the vector and add values at corresponding index.\nTherefore, the overall time complexity should be O(n)* O(n) + O(n) = O(n^2) + O(n) = O(n^2)"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "An example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog lol"
  },
  {
    "objectID": "posts/Blog_Post_Gradient_Descent/Blog_Post_Gradient_Descent.html",
    "href": "posts/Blog_Post_Gradient_Descent/Blog_Post_Gradient_Descent.html",
    "title": "Blog Post: Gradient Descent",
    "section": "",
    "text": "Implementation of Logistic Regression with Gradient Descent\n\nGradient descent update of weight vector w follows this formula:\n\\[w^{(t+1)} \\leftarrow w^{(t)} - \\alpha \\nabla L(w^{(t)})\\]\n\nCompute gradient of the empirical risk for logistic regression \\(\\nabla L(w)\\)\nAs shown in the class notes, \\[\\nabla L(w) = \\frac{1}{n} * \\sum \\limits _{i=1} ^{n} (\\sigma (<w, x_i>) - y_i)*x_i \\] We can calculate the gradient according to this formula, as illustrated in the gradient_descent function in the source code.\nFor every step in the updating process:\nWe subtract \\(\\alpha \\nabla L(w^{(t)})\\) from the current weight vector $ w^{(t)}$ to update the weight vector w until convergence.\n\n\n\nLoss history update follows this formula:\n\\[ l(\\hat{y}, y) = -ylog\\sigma(\\hat{y}) - (1-y)log(1-\\sigma(\\hat{y})) \\]\nwhere\n\\[ \\sigma (\\hat{y}) = \\frac{1}{1+e^{-\\hat{y}}} \\]\nWe just need to compute the average value of the loss function and append them to the loss_history list.\n\n\nStochastic gradient descent update:\nThe stochastic gradient descent follows the formula:\n\\[ \\nabla_S L(w) = \\frac{1}{|S|} \\sum \\limits_{i \\in S} \\nabla \\sigma (<w, x_i>) - y_i)*x_i \\]\nWe just need to compute the stochastic gradient descent and use it to update the weight vector as suggested by \\(w^{(t+1)} \\leftarrow w^{(t)} - \\alpha \\nabla L(w^{(t)})\\).\n\n\nMomentum (optional):\nMomentum is an optional variable \\(\\beta\\) that could possibly accelerate the program. According to p.85 of Hardt and Recht,\n\\[w_{k+1} = w_k - \\alpha_kg_k(w_k)+ \\beta(w_k - w_{k-1}) \\]\nTo implement momentum in fit_stochastic(), we need some variable that stores the previous \\(w\\): \\((w_{k-1})\\) and current \\(w\\): \\((w_k)\\) to compute the updated \\(w\\): (\\(w_{k+1}\\)).\n\nthe fit() function:\n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n# fit the model\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.1, max_epochs = 100)\n\n# inspect the fitted value of w\nprint(LR.w)\nprint(LR.loss_history[-10:])\nprint(LR.score_history[-10:])\n\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n# fit()\n\nloss = LR.loss(X, y)\n\nfig, axarr = plt.subplots(1, 2)\n\naxarr[0].scatter(X[:,0], X[:,1], c = y)\naxarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Loss = {loss}\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = axarr[0].plot(f1, (LR.w[2] - f1*LR.w[0])/LR.w[1], color = \"black\")\n\naxarr[1].plot(LR.loss_history)\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\nplt.tight_layout()\n\n[1.4488913  1.1878909  0.09130459]\n[0.20565486085080245, 0.20524657060908605, 0.20484531938583295, 0.204450929824252, 0.20406323050036534, 0.20368205567548014, 0.20330724506101988, 0.20293864359499791, 0.20257610122946323, 0.20221947272829266]\n[0.82, 0.82, 0.82, 0.82, 0.82, 0.82, 0.82, 0.83, 0.83, 0.83]\n\n\n\n\n\n\n\n\n\n\nthe fit_stochastic() function without momentum:\n\n# make the data for stochastic\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n# fit the model\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, alpha = 0.1, momentum = False, batch_size = 10, max_epochs = 100)\n\n# inspect the fitted value of w\n#LR.w \nprint(LR.loss_history[-10:])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n# fit_stochastic()\n\nloss = LR.loss(X, y)\n\nfig, axarr = plt.subplots(1, 2)\n\naxarr[0].scatter(X[:,0], X[:,1], c = y)\naxarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Loss = {loss}\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = axarr[0].plot(f1, (LR.w[2] - f1*LR.w[0])/LR.w[1], color = \"black\")\n\naxarr[1].plot(LR.loss_history)\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\nplt.tight_layout()\n\n[0.2259531715343903, 0.22595564897657913, 0.22595554249826233, 0.22595321577323918, 0.22595213303743808, 0.2259497420629399, 0.22595184551921593, 0.2259550169688368, 0.22596294630154048, 0.2259562448794485]\n\n\n\n\n\n\n\n\n\n\nfit_stochastic() function with momentum:\n\n# fit the model\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, alpha = 0.1, momentum = True, batch_size = 10, max_epochs = 100)\n\n# inspect the fitted value of w\n#LR.w \nprint(LR.loss_history[-10:])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n# fit_stochastic () with momentum\n\nloss = LR.loss(X, y)\n\nfig, axarr = plt.subplots(1, 2)\n\naxarr[0].scatter(X[:,0], X[:,1], c = y)\naxarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Loss = {loss}\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = axarr[0].plot(f1, (LR.w[2] - f1*LR.w[0])/LR.w[1], color = \"black\")\n\naxarr[1].plot(LR.loss_history)\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\nplt.tight_layout()\n\n[0.22597989694657847, 0.22618024352175845, 0.22629191258513107, 0.22608048805295794, 0.2260803900695497, 0.22604297177583932, 0.22599096301984894, 0.22617212604896203, 0.22607907289847376, 0.2259618004982282]\n\n\n\n\n\n\n\n\n\n\n\n\nExperiments\n\nfrom gradient_descent import LogisticRegression\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\n{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}\n\n\n\nCase 1: Gradient Descent does not converge to a minimizer because the learning rate \\(\\alpha\\) is too large\n\n# make the data\np_features = 10\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  momentum = True, \n                  batch_size = 10, \n                  alpha = .99) \n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient (momentum)\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = .99)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .99, max_epochs = 100)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\n\n\n\nWhen learning rate \\(\\alpha\\) is very large (close to 1), it actually converges. I think there might be something wrong in the description of experiment cases. I think that if learning rate is very small, it might not be able to converge to minimizer, as shown below:\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  momentum = True, \n                  batch_size = 10, \n                  alpha = .00001) \n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient (momentum)\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = .00001)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .00001, max_epochs = 100)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\n\n\n\nWe can see that the learning rate is too small that the gradient descent does not converge to a minimizer.\n\n\nCase 2: The choices of batch size influences how quickly the algorithm converges.\n\n# make the data\np_features = 10\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 1000, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = .05) \n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient with batch size = 10\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 1000, \n                  momentum = False, \n                  batch_size = 50, \n                  alpha = .05)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient with batch size = 50\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 1000, \n                  momentum = False, \n                  batch_size = 100, \n                  alpha = .05)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient with batch size = 100\")\n\n\nplt.loglog()\n\nlegend = plt.legend()\n\n\n\n\nIt seems that the function with smaller batch size converges faster.\n\n\nCase 3: Comparing the use of momentum to see if it speeds up the convergence\n\n# make the data\np_features = 10\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  momentum = True, \n                  batch_size = 10, \n                  alpha = .05) \n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient (momentum)\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = .05)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .05, max_epochs = 100)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\n\n\n\nAfter trying several times, it seems that function with momentum seems to converge faster than the function using normal stochastic gradient and normal empirical risk gradient."
  },
  {
    "objectID": "posts/Blog_Post_Kernel_Logistic_Regression/Blog_Post_Kernel_Logistic_Regression.html",
    "href": "posts/Blog_Post_Kernel_Logistic_Regression/Blog_Post_Kernel_Logistic_Regression.html",
    "title": "Blog Post: Kernel Logistic Regression",
    "section": "",
    "text": "Implementation of Logistic Regression with Kernel Function\n\nEmpirical risk and gradient descent update of weight vector v follows this formula:\n\\[v^{(t+1)} \\leftarrow v^{(t)} - \\alpha \\nabla L(v^{(t)})\\]\n\nCompute gradient of the empirical risk for logistic regression \\(\\nabla L(v)\\)\nAs shown in blog post instruction, the empirical risk of \\(L(v)\\) is: \\[L(v) = \\frac{1}{n} * \\sum \\limits _{i=1} ^{n} l(<v, k(x_i)>,y_i) \\]\nWe can then calculate the gradient of empirical risk function, \\[\\nabla L(v) = \\frac{1}{n} * \\sum \\limits _{i=1} ^{n} (\\sigma (<v, k(x_i)>) - y_i)*k(x_i) \\] We can calculate the gradient according to this formula, as illustrated in the empirical_risk and logistic_loss function in the source code.\nFor every step in the updating process:\nWe subtract \\(\\alpha \\nabla L(v^{(t)})\\) from the current weight vector $ v^{(t)}$ to update the weight vector v until convergence.\n\n\n\nLoss history update follows this formula:\n\\[ l(\\hat{y}, y) = -ylog\\sigma(\\hat{y}) - (1-y)log(1-\\sigma(\\hat{y})) \\]\nwhere\n\\[\\hat{y} = <\\hat{v}, k(x)> \\]\n\\[ \\sigma (\\hat{y}) = \\frac{1}{1+e^{-\\hat{y}}} \\]\n\\(\\hat{v}\\) refers to the optimal \\(\\hat{v}\\) obtained from training the model, and can be used to make prediction about \\(\\hat{y}\\).\nWe just need to compute the average value of the loss function and append them to the loss_history list.\n\n\n\nCheck and Experiments\n\nBasic Check\n\nfrom kernel_logistic import KernelLogisticRegression # your source code\nfrom sklearn.metrics.pairwise import rbf_kernel\nfrom sklearn.datasets import make_moons, make_circles\nfrom sklearn.linear_model import LogisticRegression\nfrom mlxtend.plotting import plot_decision_regions\n\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\nnp.seterr(all=\"ignore\")\n\nX, y = make_moons(200, shuffle = True, noise = 0.2)\nplt.scatter(X[:,0], X[:,1], c = y)\nlabels = plt.gca().set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\nKLR = KernelLogisticRegression(rbf_kernel, gamma = .1)\nKLR.fit(X, y)\nprint(KLR.score(X, y))\n\n0.88\n\n\n\n\n\n\n\nChoosing Gamma\n\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 50000)\nKLR.fit(X, y, max_epochs = 10000, alpha = 1)\nprint(KLR.score(X, y))\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n1.0\n\n\n\n\n\n\nplt.plot(KLR.loss_history)\n\n\n\n\n\n\nGenerating some new data and see the performance\n\n# new data with the same rough pattern\nX, y = make_moons(200, shuffle = True, noise = 0.2)\nplot_decision_regions(X, y, clf = KLR)\ntitle = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nIt can be seen from the above figure that the accuracy of the model decreases from 1.0 to 0.875, and the little orange blobs were not around each orange data point, indicating a worse performance. We say that the validation or testing accuracy of the classifier is quite low, and the model is overfitting.\n\n\nVarying Gamma\n\nExperiment 1: Gamma = 10\n\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 10)\nKLR.fit(X, y, max_epochs = 10000, alpha = 1)\nprint(KLR.score(X, y))\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n0.995\n\n\n\n\n\n\n# new data with the same rough pattern\nX, y = make_moons(200, shuffle = True, noise = 0.2)\nplot_decision_regions(X, y, clf = KLR)\ntitle = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\n\n\nExperiment 2: Gamma = 1000\n\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 1000)\nKLR.fit(X, y, max_epochs = 10000, alpha = 1)\nprint(KLR.score(X, y))\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n1.0\n\n\n\n\n\n\n# new data with the same rough pattern\nX, y = make_moons(200, shuffle = True, noise = 0.2)\nplot_decision_regions(X, y, clf = KLR)\ntitle = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\n\n\nExperiment 3: Gamma = 10000\n\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 10000)\nKLR.fit(X, y, max_epochs = 10000, alpha = 1)\nprint(KLR.score(X, y))\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n1.0\n\n\n\n\n\n\n# new data with the same rough pattern\nX, y = make_moons(200, shuffle = True, noise = 0.2)\nplot_decision_regions(X, y, clf = KLR)\ntitle = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\n\n\nExperiment 4: Gamma = 100000\n\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 50000)\nKLR.fit(X, y, max_epochs = 10000, alpha = 1)\nprint(KLR.score(X, y))\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n1.0\n\n\n\n\n\n\n# new data with the same rough pattern\nX, y = make_moons(200, shuffle = True, noise = 0.2)\nplot_decision_regions(X, y, clf = KLR)\ntitle = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nIn general, from the experiments with different gamma, there seems to be a pattern that with higher gamma, the model fits the data in more detailed manner, therefore higher gamma has greater probability of causing overfitting.\n\n\n\nVarying the Noise\n\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 50000)\nKLR.fit(X, y, max_epochs = 10000, alpha = 1)\nprint(KLR.score(X, y))\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n1.0\n\n\n\n\n\n\nExperiment 5: noise = 0.1\n\n# new data with the same rough pattern\nX, y = make_moons(200, shuffle = True, noise = 0.1)\nplot_decision_regions(X, y, clf = KLR)\ntitle = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\n\n\nExperiment 6: noise = 0.5\n\n# new data with the same rough pattern\nX, y = make_moons(200, shuffle = True, noise = 0.5)\nplot_decision_regions(X, y, clf = KLR)\ntitle = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\n\n\nExperiment 7: noise = 0.8\n\n# new data with the same rough pattern\nX, y = make_moons(200, shuffle = True, noise = 0.8)\nplot_decision_regions(X, y, clf = KLR)\ntitle = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nWe can see from the above graph that as noise increases, the data spreads in a more random way. Data with different labels almost mixed together in this plot, therefore leading to a lower accuracy in predicting. Higher gamma tends to overfit, therefore when the noise is high, it is better to use a model with lower gamma to avoid overfitting.\n\n\n\nMake circles\n\nnoise = 0.05\n\nX, y = make_circles(200, shuffle = True, noise = 0.05)\nplt.scatter(X[:,0], X[:,1], c = y)\nlabels = plt.gca().set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\nKLR = KernelLogisticRegression(rbf_kernel, gamma = .1)\nKLR.fit(X, y)\nprint(KLR.score(X, y))\n\n0.5\n\n\n\n\n\n\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 500)\nKLR.fit(X, y, max_epochs = 10000, alpha = 1)\nprint(KLR.score(X, y))\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n0.995\n\n\n\n\n\n\n\nnoise = 0.2\n\nX, y = make_circles(200, shuffle = True, noise = 0.2)\nplt.scatter(X[:,0], X[:,1], c = y)\nlabels = plt.gca().set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\nKLR = KernelLogisticRegression(rbf_kernel, gamma = .2)\nKLR.fit(X, y)\nprint(KLR.score(X, y))\n\n0.5\n\n\n\n\n\n\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 500)\nKLR.fit(X, y, max_epochs = 10000, alpha = 1)\nprint(KLR.score(X, y))\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n1.0\n\n\n\n\n\n\n\nnoise = 0.8\n\nX, y = make_circles(200, shuffle = True, noise = 0.2)\nplt.scatter(X[:,0], X[:,1], c = y)\nlabels = plt.gca().set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\nKLR = KernelLogisticRegression(rbf_kernel, gamma = .2)\nKLR.fit(X, y)\nprint(KLR.score(X, y))\n\n0.5\n\n\n\n\n\n\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 500)\nKLR.fit(X, y, max_epochs = 10000, alpha = 1)\nprint(KLR.score(X, y))\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n0.99\n\n\n\n\n\nFrom the experiment on make_circles, it seems that when noise gets higher, the data spreads more, so when we try to fit the model, it is hard for us to get high accuracy if we are having a low gamma value. I think from all the experiments I made about make_circles, the one with noise = 0.05 and gamma = 500 matches is a pretty good prediction, as it is not too detailed (overfitting) and still has high accuracy."
  },
  {
    "objectID": "posts/Blog_Post_Linear_Regression/Blog_Post_Linear_Regression.html",
    "href": "posts/Blog_Post_Linear_Regression/Blog_Post_Linear_Regression.html",
    "title": "Blog Post: Linear Regression",
    "section": "",
    "text": "Creating and Plotting Data\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\n\n\n\n\n\n\nPart I: Implementing Linear Regression in Two Ways\n\nMethod 1: Using analytical formula\nIn the lecture notes, we can find an analytical formula that can calculate the optimal weight vector \\(\\hat{w}\\):\n\\[\\hat{w} = (X^{T}X)^{-1}X^{T}y\\]\nTherefore, we can use the formula w_hat = np.linalg.inv(X.T@X)@X.T@y to compute the optimal weight vector \\(\\hat{w}\\) directly. The method is denoted as fit_analytical(X, y) in linear_regression.py\n\nApplication of Method 1\n\nfrom linear_regression import LinearRegression\nLR = LinearRegression()\nLR.fit_analytical(X_train, y_train) # I used the analytical formula as my default fit method\n\nprint(f\"Training score = {LR.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.5105\nValidation score = 0.586\n\n\n\n\nGet optimal weight vector\n\nLR.w\n\narray([0.72338107, 0.82081102])\n\n\n\n\n\nMethod 2: Using gradient descent method\nThis method basically follows the gradient descent method as other blog posts.\nThe formula of the gradient is: \\[\\nabla L(w) = 2X^{T}(Xw-y)\\]\nFor every step in the updating process:\nWe subtract \\(\\alpha \\nabla L(w^{(t)})\\) from the current weight vector $ w^{(t)}$ to update the weight vector w until convergence.\nThis method is denoted as fit_gradient(X, y) in linear_regression.py\n\nApplication of Method 2:\n\nLR2 = LinearRegression()\n\nLR2.fit_gradient(X_train, y_train, alpha = 0.01, max_iter = 1e2)\nLR2.w\n\narray([0.81740246, 0.75477255])\n\n\n\nplt.plot(LR2.score_history)\nlabels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\")\n\n\n\n\n\n\n\n\nPart II: Experiments\nThis experiment requires to increase p_features, the number of features used, while holding n_train, the number of training points, constant.\nIn this experiment, p_features will be increased all the way to n_train - 1. A plot will be made to illustrate the change of scores corresponding to the increase of p_features.\n\nn_train = 100\nn_val = 100\nnoise = 0.2\n\ntraining_score_history = []\nvalidation_score_history = []\np_features = 1\nfor i in range (n_train-1):\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    LR.fit_analytical(X_train, y_train)\n    training_score_history.append(LR.score(X_train, y_train).round(4))\n    validation_score_history.append(LR.score(X_val, y_val).round(4))\n    p_features += 1\nplt.plot(np.arange(n_train-1), training_score_history, label = \"Training Score\")\nplt.plot(np.arange(n_train-1), validation_score_history, label = \"Validation Score\")\nlabels = plt.gca().set(xlabel = \"p_features\", ylabel = \"Scores\")\nlegend = plt.legend()\n\n\n\n\n\nObservation:\nThe training score does not change much, but the validation score experienced a huge drop and rise (large-scale fluctuation) when p_features is large. This might be due to the model is overfitting to the training dataset, therefore it is not able to predict the validation set very well. This causes a stark decrease in the validation score.\n\n\n\nPart III: LASSO Regularization\nIn this part, I will use the LASSO algorithm instead of the regular linear regression algorithm to fit the model.\nThe LASSO algorithm uses a modified loss function with a regularization term:\n\\[L(w) = ||Xw-y||_2^{2} + \\alpha ||w'||_1\\]\nwhere w’ is the vector composed of all entries of w exclusing the very last entry.\nIn LASSO algorithm, the entries of the weight vector w is very small, tending to force entries of the weight vector to be exactly zero.\n\nExperiment on LASSO Algorithm\nIn this experiment, I will use similar method as used in Part II. I will increase the p_features all the way to n_train -1 to see how scores change along the way.\n\nExperiment 1: alpha = 0.001\n\nfrom sklearn.linear_model import Lasso\nL = Lasso(alpha = 0.001)\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\nL_training_score_history = []\nL_validation_score_history = []\nfor i in range (n_train-1):\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    L.fit(X_train, y_train)\n    L_training_score_history.append(L.score(X_train, y_train).round(4))\n    L_validation_score_history.append(L.score(X_val, y_val).round(4))\n    p_features += 1\nplt.plot(np.arange(n_train-1), L_training_score_history, label = \"Lasso Training Score\")\nplt.plot(np.arange(n_train-1), L_validation_score_history, label = \"Lasso Validation Score\")\nlabels = plt.gca().set(xlabel = \"p_features\", ylabel = \"Scores\")\nlegend = plt.legend()\n\n\n\n\n\n\nExperiment 2: alpha = 0.01\n\nfrom sklearn.linear_model import Lasso\nL = Lasso(alpha = 0.01)\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\nL_training_score_history = []\nL_validation_score_history = []\nfor i in range (n_train-1):\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    L.fit(X_train, y_train)\n    L_training_score_history.append(L.score(X_train, y_train).round(4))\n    L_validation_score_history.append(L.score(X_val, y_val).round(4))\n    p_features += 1\nplt.plot(np.arange(n_train-1), L_training_score_history, label = \"Lasso Training Score\")\nplt.plot(np.arange(n_train-1), L_validation_score_history, label = \"Lasso Validation Score\")\nlabels = plt.gca().set(xlabel = \"p_features\", ylabel = \"Scores\")\nlegend = plt.legend()\n\n\n\n\n\n\nExperiment 3: alpha = 0.1\n\nfrom sklearn.linear_model import Lasso\nL = Lasso(alpha = 0.1)\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\nL_training_score_history = []\nL_validation_score_history = []\nfor i in range (n_train-1):\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    L.fit(X_train, y_train)\n    L_training_score_history.append(L.score(X_train, y_train).round(4))\n    L_validation_score_history.append(L.score(X_val, y_val).round(4))\n    p_features += 1\nplt.plot(np.arange(n_train-1), L_training_score_history, label = \"Lasso Training Score\")\nplt.plot(np.arange(n_train-1), L_validation_score_history, label = \"Lasso Validation Score\")\nlabels = plt.gca().set(xlabel = \"p_features\", ylabel = \"Scores\")\nlegend = plt.legend()\n\n\n\n\n\n\nExperiment 4: alpha = 0.8\n\nfrom sklearn.linear_model import Lasso\nL = Lasso(alpha = 0.8)\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\nL_training_score_history = []\nL_validation_score_history = []\nfor i in range (n_train-1):\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    L.fit(X_train, y_train)\n    L_training_score_history.append(L.score(X_train, y_train).round(4))\n    L_validation_score_history.append(L.score(X_val, y_val).round(4))\n    p_features += 1\nplt.plot(np.arange(n_train-1), L_training_score_history, label = \"Lasso Training Score\")\nplt.plot(np.arange(n_train-1), L_validation_score_history, label = \"Lasso Validation Score\")\nlabels = plt.gca().set(xlabel = \"p_features\", ylabel = \"Scores\")\nlegend = plt.legend()\n\n\n\n\n\n\nObservation:\nAbout alpha:\nIt seems that with increasing alpha, the fluctuation of validation score became more obvious and stronger.\nAbout validation score:\nCompare to the result I get from performing standard linear regression when the number of features is large, I can see that the level of change of validation score is milder for LASSO algorithm than the standard linear regression. I think this might be due to the regularization, which makes the w to be at a relatively small value."
  },
  {
    "objectID": "posts/Blog_Post_Timnit_Gebru_Visit/Blog_Post_TG.html",
    "href": "posts/Blog_Post_Timnit_Gebru_Visit/Blog_Post_TG.html",
    "title": "Blog Post: Learning from Timnit Gebru",
    "section": "",
    "text": "What is Happening at Middlebury\nMiddlebury offers courses such as Artificial Intelligence and Machine Learning and regularly discusses topics related to ethics in AI.\nLike many other colleges, Middlebury is also facing the impact of ChatGPT. According to the Middlebury Campus Newspaper, the college has yet to establish a campus-wide policy regarding the use of ChatGPT or other AI tools, leaving the decision of whether to allow or prohibit such tools up to individual professors. However, there are ethical concerns associated with the use of ChatGPT and other AI tools. For instance, AI tools like ChatGPT and Grammarly have created a divide among students between those who can afford the premium version versus those who cannot, providing a significant advantage to students who can pay for premium features or information. Furthermore, large language models such as ChatGPT are trained on massive amounts of language data, which includes violent, racist, and sexist messages that people share online. Although developers try to mitigate such biased outputs before releasing ChatGPT, the chatbot still generates subtle and overt forms of bias.\n\n\nSummary of Dr. Timnit Gebru’s Talk\nIn her talk, Dr. Timnit Gebru discussed the current applications of computer vision technology and expressed her concerns about their unreliability and bias. She highlighted examples such as Faception and Hirevue, which use facial images to judge people’s personalities and emotions. Dr. Gebru emphasized that these measures cannot truly reflect a person’s true pattern and, to a great extent, perpetuate structural racism. Faception, in particular, singles out individuals from marginalized groups as terrorists.\nDr. Gebru also commented on the tendency of fields such as machine learning, computer vision, and data sciences to abstract things, which she believed was problematic since the application of AI is intended to deal with real people. She quoted Seeta Gangadharan’s statement that “papers tend to disappear people into mathematical equations,” highlighting the importance of considering the human aspect when designing and implementing AI.\nThe lack of diversity in datasets was another issue raised by Dr. Gebru. She cited her research on Gender Shades, which found that facial analysis datasets were predominantly composed of lighter-skinned individuals and males, leading to a high disparity in facial recognition results. Dr. Gebru also noted that while many people advocate for diversity in datasets, it is not enough to make things work equally well for everyone. Even if it does work for everyone, the situation can still be problematic. She discussed how some companies and institutions unethically gather “diverse” information by predatorily seeking more darker skin images and scraping images of transgender YouTubers without their consent.\nMoreover, Dr. Gebru stressed that technology is not always used in the way it is designed, and sometimes its use leads to more discrimination and problems than efficiency. People often rely too much on technology tools that ignore the real facts presented to them.\nDr. Gebru suggested that we need to think beyond diversity in datasets and consider structural and real representation. She highlighted that panels mainly comprise individuals from dominant groups and those closest to money, which can marginalize vulnerable groups. She concluded that fairness is not only about datasets or math but also about society, and computer scientists should not shy away from this fact.\n\n\nQuestions to Dr. Gebru\n\nConsidering the unreliable and biased nature of facial recognition tools, do you think it is necessary to prohibit or regulate the use of facial recognition on people, or to focus on improving the accuracy of the model and taking into account social factors?\nHow can computer vision technology assist marginalized communities, and what level of regulation do you believe is necessary?"
  },
  {
    "objectID": "posts/Blog_Post_Auditing_Allocative_Bias/Blog_Post_Auditing_Allocative_Bias.html",
    "href": "posts/Blog_Post_Auditing_Allocative_Bias/Blog_Post_Auditing_Allocative_Bias.html",
    "title": "Blog Post: Auditing Allocative Bias",
    "section": "",
    "text": "Loading the data\n\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\n\nSTATE = \"TX\" # pick Texas as the state\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\nacs_data.head()\n\n\n\n\n\n  \n    \n      \n      RT\n      SERIALNO\n      DIVISION\n      SPORDER\n      PUMA\n      REGION\n      ST\n      ADJINC\n      PWGTP\n      AGEP\n      ...\n      PWGTP71\n      PWGTP72\n      PWGTP73\n      PWGTP74\n      PWGTP75\n      PWGTP76\n      PWGTP77\n      PWGTP78\n      PWGTP79\n      PWGTP80\n    \n  \n  \n    \n      0\n      P\n      2018GQ0000026\n      7\n      1\n      5000\n      3\n      48\n      1013097\n      29\n      21\n      ...\n      29\n      4\n      5\n      54\n      27\n      52\n      28\n      29\n      52\n      29\n    \n    \n      1\n      P\n      2018GQ0000057\n      7\n      1\n      6601\n      3\n      48\n      1013097\n      16\n      19\n      ...\n      33\n      3\n      18\n      16\n      32\n      3\n      18\n      16\n      2\n      2\n    \n    \n      2\n      P\n      2018GQ0000070\n      7\n      1\n      4302\n      3\n      48\n      1013097\n      64\n      24\n      ...\n      14\n      64\n      110\n      62\n      14\n      15\n      64\n      64\n      13\n      67\n    \n    \n      3\n      P\n      2018GQ0000079\n      7\n      1\n      700\n      3\n      48\n      1013097\n      260\n      20\n      ...\n      57\n      451\n      261\n      272\n      59\n      477\n      261\n      258\n      480\n      56\n    \n    \n      4\n      P\n      2018GQ0000082\n      7\n      1\n      900\n      3\n      48\n      1013097\n      12\n      31\n      ...\n      10\n      3\n      11\n      22\n      12\n      20\n      1\n      12\n      21\n      1\n    \n  \n\n5 rows × 286 columns\n\n\n\nKeep possible feature that may be used for analysis\n\npossible_features=['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\nacs_data[possible_features].head()\n\n\n\n\n\n  \n    \n      \n      AGEP\n      SCHL\n      MAR\n      RELP\n      DIS\n      ESP\n      CIT\n      MIG\n      MIL\n      ANC\n      NATIVITY\n      DEAR\n      DEYE\n      DREM\n      SEX\n      RAC1P\n      ESR\n    \n  \n  \n    \n      0\n      21\n      16.0\n      5\n      17\n      2\n      NaN\n      1\n      1.0\n      4.0\n      1\n      1\n      2\n      2\n      2.0\n      2\n      2\n      1.0\n    \n    \n      1\n      19\n      16.0\n      5\n      17\n      2\n      NaN\n      1\n      1.0\n      4.0\n      1\n      1\n      2\n      2\n      2.0\n      2\n      1\n      6.0\n    \n    \n      2\n      24\n      12.0\n      5\n      16\n      1\n      NaN\n      1\n      1.0\n      4.0\n      1\n      1\n      2\n      2\n      1.0\n      1\n      2\n      6.0\n    \n    \n      3\n      20\n      16.0\n      5\n      17\n      2\n      NaN\n      1\n      1.0\n      3.0\n      1\n      1\n      2\n      2\n      2.0\n      2\n      1\n      1.0\n    \n    \n      4\n      31\n      17.0\n      5\n      17\n      2\n      NaN\n      1\n      3.0\n      4.0\n      4\n      1\n      2\n      2\n      2.0\n      1\n      1\n      1.0\n    \n  \n\n\n\n\n\nfeatures_to_use = [f for f in possible_features if f not in [\"ESR\", \"RAC1P\"]]\n\n\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data)\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\n\n\nBasic Descriptives\n\nimport pandas as pd\ndf = pd.DataFrame(X_train, columns = features_to_use)\ndf[\"group\"] = group_train\ndf[\"label\"] = y_train\n\n\nQuestion 1: How many individuals are in the data?\n\ntotal = len(df.index)\ntotal\n\n214480\n\n\nWe can see that there are 214480 individuals in the data.\n\n\nQuestion 2: Of these individuals, what proportion have target label equal to 1? In employment prediction, these would correspond to employed individuals.\n\ntotal_1 = len(df[df['label'] == 1])\nproportion_1 = total_1 / total\nproportion_1\n\n0.4513474449832152\n\n\nOf these individuals, approximately 45% of the individuals have target label equal to 1.\n\n\nQuestion 3: Of these individuals, how many are in each of the groups?\n\ntarget_1 = df[df['label'] == 1]\ntarget_1_groups = target_1.groupby(['group'])['group'].count()\nprint(target_1_groups)\n\ngroup\n1    75556\n2     8591\n3      361\n4        5\n5      185\n6     5064\n7       68\n8     4921\n9     2054\nName: group, dtype: int64\n\n\nNumber of people having target label equal to 1 is shown in the above table\n\n\nQuestion 4: In each group, what proportion of individuals have target label equal to 1?\n\ntarget_groups_prop = df.groupby(['group'])['label'].mean()\nprint(target_groups_prop)\n\ngroup\n1    0.455242\n2    0.416756\n3    0.431818\n4    0.357143\n5    0.461347\n6    0.499359\n7    0.430380\n8    0.466711\n9    0.353955\nName: label, dtype: float64\n\n\n\n\nQuestion 5: Adding another label “SEX”\n\ntarget_groups_with_sex = df.groupby(['group', 'SEX'])['label'].mean()\nprint(target_groups_with_sex)\n\ngroup  SEX\n1      1.0    0.502327\n       2.0    0.410106\n2      1.0    0.396673\n       2.0    0.435596\n3      1.0    0.474178\n       2.0    0.387805\n4      1.0    0.250000\n       2.0    0.500000\n5      1.0    0.488263\n       2.0    0.430851\n6      1.0    0.553741\n       2.0    0.447318\n7      1.0    0.428571\n       2.0    0.432836\n8      1.0    0.529160\n       2.0    0.401550\n9      1.0    0.371400\n       2.0    0.335905\nName: label, dtype: float64\n\n\nMaking Barplot:\n\n# make barplot\nimport seaborn as sns\n\nresult = target_groups_with_sex.to_frame().reset_index()\nsns.barplot(x='group', y = 'label', hue = 'SEX', data = result)\n\n<AxesSubplot: xlabel='group', ylabel='label'>\n\n\n\n\n\n\n\n\nTraining the Model\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\n\nmodel = make_pipeline(StandardScaler(), LogisticRegression())\nmodel.fit(X_train, y_train)\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('logisticregression', LogisticRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('standardscaler', StandardScaler()),\n                ('logisticregression', LogisticRegression())])StandardScalerStandardScaler()LogisticRegressionLogisticRegression()\n\n\n\ny_hat = model.predict(X_test)\n\n\n(y_hat == y_test).mean()\n\n0.7754569190600522\n\n\n\n\nAuditing the Model\n\nOverall Measures\n\nWhat is the overall accuracy of your model?\nAs the result of the training model section shows, the overall accuracy of my model is 0.7754569190600522.\n\n\nWhat is the positive predictive value (PPV) of your model?\n\ndf_test = pd.DataFrame(X_test, columns = features_to_use)\n\ndf_test[\"group\"] = group_test\ndf_test[\"label\"] = y_test\ndf_test[\"predicted\"] = y_hat\ndf_test[\"match\"] = (df_test[\"predicted\"] == y_test)\ndf_test\n\n\n\n\n\n  \n    \n      \n      AGEP\n      SCHL\n      MAR\n      RELP\n      DIS\n      ESP\n      CIT\n      MIG\n      MIL\n      ANC\n      NATIVITY\n      DEAR\n      DEYE\n      DREM\n      SEX\n      group\n      label\n      predicted\n      match\n    \n  \n  \n    \n      0\n      3.0\n      1.0\n      5.0\n      2.0\n      2.0\n      2.0\n      1.0\n      1.0\n      0.0\n      1.0\n      1.0\n      2.0\n      2.0\n      0.0\n      2.0\n      8\n      False\n      False\n      True\n    \n    \n      1\n      30.0\n      19.0\n      4.0\n      2.0\n      2.0\n      0.0\n      5.0\n      1.0\n      4.0\n      1.0\n      2.0\n      2.0\n      2.0\n      2.0\n      2.0\n      1\n      True\n      True\n      True\n    \n    \n      2\n      5.0\n      2.0\n      5.0\n      2.0\n      2.0\n      7.0\n      1.0\n      1.0\n      0.0\n      1.0\n      1.0\n      2.0\n      2.0\n      2.0\n      2.0\n      1\n      False\n      False\n      True\n    \n    \n      3\n      92.0\n      19.0\n      2.0\n      16.0\n      1.0\n      0.0\n      1.0\n      1.0\n      4.0\n      1.0\n      1.0\n      1.0\n      2.0\n      2.0\n      2.0\n      1\n      False\n      False\n      True\n    \n    \n      4\n      58.0\n      15.0\n      1.0\n      1.0\n      2.0\n      0.0\n      1.0\n      1.0\n      4.0\n      1.0\n      1.0\n      2.0\n      2.0\n      2.0\n      1.0\n      1\n      True\n      True\n      True\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      53615\n      38.0\n      22.0\n      1.0\n      0.0\n      2.0\n      0.0\n      1.0\n      3.0\n      4.0\n      2.0\n      1.0\n      2.0\n      2.0\n      2.0\n      1.0\n      1\n      True\n      True\n      True\n    \n    \n      53616\n      23.0\n      11.0\n      5.0\n      2.0\n      2.0\n      0.0\n      5.0\n      1.0\n      4.0\n      1.0\n      2.0\n      2.0\n      2.0\n      2.0\n      1.0\n      1\n      True\n      True\n      True\n    \n    \n      53617\n      44.0\n      22.0\n      1.0\n      0.0\n      2.0\n      0.0\n      4.0\n      1.0\n      4.0\n      1.0\n      2.0\n      2.0\n      2.0\n      2.0\n      2.0\n      6\n      True\n      True\n      True\n    \n    \n      53618\n      62.0\n      21.0\n      2.0\n      0.0\n      2.0\n      0.0\n      1.0\n      1.0\n      2.0\n      1.0\n      1.0\n      2.0\n      2.0\n      2.0\n      1.0\n      1\n      True\n      False\n      False\n    \n    \n      53619\n      75.0\n      19.0\n      1.0\n      0.0\n      1.0\n      0.0\n      1.0\n      1.0\n      4.0\n      1.0\n      1.0\n      1.0\n      2.0\n      2.0\n      1.0\n      1\n      True\n      False\n      False\n    \n  \n\n53620 rows × 19 columns\n\n\n\n\ncm = confusion_matrix(df_test[\"label\"], df_test[\"predicted\"])\nTP = cm[1,1]\nFP = cm[0,1]\nTN = cm[0,0]\nFN = cm[1,0]\nPPV = TP / (TP + FP)\nprint(PPV)\n\n0.7249736181361668\n\n\nPPV for this model is 0.7249736181361668\n\n\nWhat are the overall false negative and false positive rates (FNR and FPR) for your model?\n\nFNR = FN / (FN + TP)\nFPR = FP / (FP + TN)\nprint(FNR)\nprint(FPR)\n\n0.18365089121081746\n0.25870272120486054\n\n\nFalse positive rate: 0.25870272120486054\nFalse negative rate: 0.18365089121081746\n\n\n\nBy-Group Measures\n\nWhat is the accuracy of your model on each subgroup?\n\n# add a column of predicted result of each entry\n# add another column of that indicates whether the predicted results matches each entry\n\naccuracy_by_subgroups = df_test.groupby(['group'])['match'].mean()\nprint(accuracy_by_subgroups)\n\ngroup\n1    0.775486\n2    0.785095\n3    0.748768\n4    0.666667\n5    0.724771\n6    0.750307\n7    0.777778\n8    0.755204\n9    0.828311\nName: match, dtype: float64\n\n\n\n\nWhat is the PPV of your model on each subgroup?\n\ndef TP(df_test):\n    if (df_test[\"predicted\"] == 1) and (df_test[\"label\"] == 1):\n        return 1\n    else:\n        return 0\n    \ndef FP(df_test):\n    if (df_test[\"predicted\"] == 1) and (df_test[\"label\"] == 0):\n        return 1\n    else:\n        return 0\n\ndef TN(df_test):\n    if (df_test[\"predicted\"] == 0) and (df_test[\"label\"] == 0):\n        return 1\n    else:\n        return 0\n    \ndef FN(df_test):\n    if (df_test[\"predicted\"] == 0) and (df_test[\"label\"] == 1):\n        return 1\n    else:\n        return 0\n\n\ndf_test['TP'] = df_test.apply(TP, axis=1)\ndf_test['FP'] = df_test.apply(FP, axis=1)\ndf_test['TN'] = df_test.apply(TN, axis=1)\ndf_test['FN'] = df_test.apply(FN, axis=1)\ndf_test\n\n\n\n\n\n  \n    \n      \n      AGEP\n      SCHL\n      MAR\n      RELP\n      DIS\n      ESP\n      CIT\n      MIG\n      MIL\n      ANC\n      ...\n      DREM\n      SEX\n      group\n      label\n      predicted\n      match\n      TP\n      FP\n      TN\n      FN\n    \n  \n  \n    \n      0\n      3.0\n      1.0\n      5.0\n      2.0\n      2.0\n      2.0\n      1.0\n      1.0\n      0.0\n      1.0\n      ...\n      0.0\n      2.0\n      8\n      False\n      False\n      True\n      0\n      0\n      1\n      0\n    \n    \n      1\n      30.0\n      19.0\n      4.0\n      2.0\n      2.0\n      0.0\n      5.0\n      1.0\n      4.0\n      1.0\n      ...\n      2.0\n      2.0\n      1\n      True\n      True\n      True\n      1\n      0\n      0\n      0\n    \n    \n      2\n      5.0\n      2.0\n      5.0\n      2.0\n      2.0\n      7.0\n      1.0\n      1.0\n      0.0\n      1.0\n      ...\n      2.0\n      2.0\n      1\n      False\n      False\n      True\n      0\n      0\n      1\n      0\n    \n    \n      3\n      92.0\n      19.0\n      2.0\n      16.0\n      1.0\n      0.0\n      1.0\n      1.0\n      4.0\n      1.0\n      ...\n      2.0\n      2.0\n      1\n      False\n      False\n      True\n      0\n      0\n      1\n      0\n    \n    \n      4\n      58.0\n      15.0\n      1.0\n      1.0\n      2.0\n      0.0\n      1.0\n      1.0\n      4.0\n      1.0\n      ...\n      2.0\n      1.0\n      1\n      True\n      True\n      True\n      1\n      0\n      0\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      53615\n      38.0\n      22.0\n      1.0\n      0.0\n      2.0\n      0.0\n      1.0\n      3.0\n      4.0\n      2.0\n      ...\n      2.0\n      1.0\n      1\n      True\n      True\n      True\n      1\n      0\n      0\n      0\n    \n    \n      53616\n      23.0\n      11.0\n      5.0\n      2.0\n      2.0\n      0.0\n      5.0\n      1.0\n      4.0\n      1.0\n      ...\n      2.0\n      1.0\n      1\n      True\n      True\n      True\n      1\n      0\n      0\n      0\n    \n    \n      53617\n      44.0\n      22.0\n      1.0\n      0.0\n      2.0\n      0.0\n      4.0\n      1.0\n      4.0\n      1.0\n      ...\n      2.0\n      2.0\n      6\n      True\n      True\n      True\n      1\n      0\n      0\n      0\n    \n    \n      53618\n      62.0\n      21.0\n      2.0\n      0.0\n      2.0\n      0.0\n      1.0\n      1.0\n      2.0\n      1.0\n      ...\n      2.0\n      1.0\n      1\n      True\n      False\n      False\n      0\n      0\n      0\n      1\n    \n    \n      53619\n      75.0\n      19.0\n      1.0\n      0.0\n      1.0\n      0.0\n      1.0\n      1.0\n      4.0\n      1.0\n      ...\n      2.0\n      1.0\n      1\n      True\n      False\n      False\n      0\n      0\n      0\n      1\n    \n  \n\n53620 rows × 23 columns\n\n\n\n\ndf_test[\"PPV\"] = df_test[\"TP\"] / (df_test[\"TP\"] + df_test[\"FP\"])\nPPV_by_subgroups = df_test.groupby([\"group\"])[\"PPV\"].mean()\nprint(PPV_by_subgroups)\n\ngroup\n1    0.726423\n2    0.729860\n3    0.697917\n4    0.666667\n5    0.722222\n6    0.697287\n7    0.769231\n8    0.723242\n9    0.737201\nName: PPV, dtype: float64\n\n\n\n\nWhat are the FNR and FPR on each subgroup?\n\ndf_test[\"FNR\"] = df_test[\"FN\"] / (df_test[\"FN\"] + df_test[\"TP\"])\nFNR_by_subgroups = df_test.groupby([\"group\"])[\"FNR\"].mean()\nprint(FNR_by_subgroups)\n\ngroup\n1    0.182327\n2    0.221028\n3    0.247191\n4    0.333333\n5    0.277778\n6    0.095200\n7    0.230769\n8    0.223954\n9    0.173996\nName: FNR, dtype: float64\n\n\n\ndf_test[\"FPR\"] = df_test[\"FP\"] / (df_test[\"FP\"] + df_test[\"TN\"])\nFPR_by_subgroups = df_test.groupby([\"group\"])[\"FPR\"].mean()\nprint(FPR_by_subgroups)\n\ngroup\n1    0.260155\n2    0.210437\n3    0.254386\n4    0.333333\n5    0.272727\n6    0.411567\n7    0.214286\n8    0.263273\n9    0.170354\nName: FPR, dtype: float64\n\n\n\n\n\nBias Measures\n\nIs your model approximately calibrated?\nFrom the PPV by subgroups data, we can see that the PPV by subgroups are generally around 0.70, ranging from 0.666 to 0.769. We can say that it seems that the model is approximately calibrated, as the PPV of the model across groups are generally within a certain range, and difference is not very significant. But still, it is not perfectly calibrated since PPV for certain groups do have some differences.\n\n\nDoes your model satisfy approximate error rate balance?\nMy model does not satisfy approximate error rate balance. To see if the model satisfy approximate error rate balance, we can measure if the FPR and FNR are equal across groups. From the FPR and FNR we calculated, it seems that for certain groups, there is big differences between the FPR and FNR, such as for group 1, FPR is 0.26, and FNR is 0.18, and for group 6, FPR is 0.41, while FNR is only 0.095, indicating that the model does not satisfy approximate error rate balance.\n\n\nDoes your model satisfy statistical parity?\n\npositive_by_subgroups = df_test.groupby(['group'])['predicted'].mean()\nprint(positive_by_subgroups)\n\ngroup\n1    0.515465\n2    0.450315\n3    0.472906\n4    0.500000\n5    0.495413\n6    0.663938\n7    0.481481\n8    0.504241\n9    0.410652\nName: predicted, dtype: float64\n\n\nMy model does not satisfy statistical parity. From the data above, we can see that there is significant difference in the rate of being predicted as positive among different groups, therefore it indicates that the proportion of individuals being predicted as positive is different across groups, so the model does not satisfy statistical parity.\n\n\n\n\nConcluding Discussion\n\nWhat groups of people could stand to benefit from a system that is able to predict the label you predicted, such as income or employment status? For example, what kinds of companies might want to buy your model for commercial use?\nCompanies who want to make the employment fairer across different races might want to buy my model, since it reflects the disparity in employment status among different races, therefore if they want to make the employment fairer for certain groups, they can use the data to find which group should be given extra consideration in the process of recruiting. Also, government that seek to improve racial equality can use this data to make policies.\n\n\nBased on your bias audit, what could be the impact of deploying your model for large-scale prediction in commercial or governmental settings?\n\nThe model may have lower accuracy or higher FPR or FNR for certain subgroups, which could result in unfair or discriminatory outcomes. If the model is used to make decisions about employment, this could lead to discrimination against certain groups.\nThe model does not necessarily satisfies statistical parity, meaning that certain groups may be systematically advantaged or disadvantaged by the model’s predictions. This could result in legal or ethical issues if the model is used for decision-making in sensitive domains.\nIf the model is found to be biased or unfair, this could result in reputational harm for the organization or government agency that deployed it. This could lead to negative impacts such as loss of trust and negative media coverage.\n\n\n\nBased on your bias audit, do you feel that your model displays problematic bias? What kind (calibration, error rate, etc)?\nI do feel that my model displays problematic bias. My model does not satisfy error rate balance and statistical parity, while the calibration is not very satisfactory as well. There exist significant disparities between certain rates (>10%), meaning that certain groups are advantaged or disdvantaged in the process.\n\n\nBeyond bias, are there other potential problems associated with deploying your model that make you uncomfortable? How would you propose addressing some of these problems?\n\ncount_by_group = df.groupby(['group']).count()\nprint(count_by_group)\n\n         AGEP    SCHL     MAR    RELP     DIS     ESP     CIT     MIG     MIL  \\\ngroup                                                                           \n1      165969  165969  165969  165969  165969  165969  165969  165969  165969   \n2       20614   20614   20614   20614   20614   20614   20614   20614   20614   \n3         836     836     836     836     836     836     836     836     836   \n4          14      14      14      14      14      14      14      14      14   \n5         401     401     401     401     401     401     401     401     401   \n6       10141   10141   10141   10141   10141   10141   10141   10141   10141   \n7         158     158     158     158     158     158     158     158     158   \n8       10544   10544   10544   10544   10544   10544   10544   10544   10544   \n9        5803    5803    5803    5803    5803    5803    5803    5803    5803   \n\n          ANC  NATIVITY    DEAR    DEYE    DREM     SEX   label  \ngroup                                                            \n1      165969    165969  165969  165969  165969  165969  165969  \n2       20614     20614   20614   20614   20614   20614   20614  \n3         836       836     836     836     836     836     836  \n4          14        14      14      14      14      14      14  \n5         401       401     401     401     401     401     401  \n6       10141     10141   10141   10141   10141   10141   10141  \n7         158       158     158     158     158     158     158  \n8       10544     10544   10544   10544   10544   10544   10544  \n9        5803      5803    5803    5803    5803    5803    5803  \n\n\nFrom this data, we can see that data for certain races is very high (as high as 165959), while data for certain racial groups is as low as 14. This could create problem since data for certain racial group is not enough, which may lead to biased and inaccurate predictions."
  },
  {
    "objectID": "posts/Blog_Post_Unsupervised_Learning/Blog_Post_Unsupervised_Learning_with_Linear_Algebra.html",
    "href": "posts/Blog_Post_Unsupervised_Learning/Blog_Post_Unsupervised_Learning_with_Linear_Algebra.html",
    "title": "Blog Post: Unsupervised Learning with Linear Algebra",
    "section": "",
    "text": "SVD Reconstruction\nThe SVD (singular value decomposition) of a m * n matrix A (the original image matrix) basically knocks off certain features (some numbers) in the original image matrix to compress the image and make it taking less storage space.\nThe \\(m * n\\) matrix D carried out through svd operation will have nonzero entries only on its main diagonal. The \\(m * m\\) matrix U and \\(n * n\\) matrix V are orthogonal matrices.\nI will define a function to show the process of using SVD to compress the image and the reconstructed image with fewer features:\n\ndef svd_reconstruct(img, k):\n    \"\"\"\n    Reconstruct the image using singular value decomposition of the image matrix\n    \"\"\"\n    U, sigma, V = np.linalg.svd(img)\n    \n    # create the D matrix in the SVD\n    D = np.zeros_like(img,dtype=float) # matrix of zeros of same shape as grey_img\n    D[:min(img.shape),:min(img.shape)] = np.diag(sigma) # singular values on the main diagonal\n    \n    # the compressed image matrix\n    U_ = U[:,:k]\n    D_ = D[:k, :k]\n    V_ = V[:k, :]\n    A_ = U_ @ D_ @ V_ # approximation of original image\n    \n    # percent of storage used\n    m = A_.shape[0]\n    n = A_.shape[1]\n    \n    p_storage = (m * k + k + k * n)/ (m*n) * 100 # since in D, only the diagonals are nonzero\n    \n    # show the reconstructed image\n    plt.imshow(A_, cmap = \"Greys\")\n    plt.axis(\"off\")\n    plt.title(str(k) + \" components\" + \"; % storage = \" + str(p_storage))\n\nWe can try this function with the greyscale image, setting components used to be k = 1, which only takes 1 column in U, 1 entry in D, and 1 row in V.\n\nsvd_reconstruct(grey_img, k = 1)\n\n\n\n\nFrom the image, we can see that this compressed image loses too many features that it is hard to tell what is in the image. We can increase the number of components and see how it goes.\n\nsvd_reconstruct(grey_img, k = 5)\n\n\n\n\nFor n = 5, it still is very vague, but there are some places in the photo that seem more specific and distinct from other parts.\n\n\nMore Experiments\nWe can conduct more experiments on SVD reconstruction, changing the component (k) along the way.\n\nsvd_reconstruct(grey_img, k = 10)\n\n\n\n\n\nsvd_reconstruct(grey_img, 20)\n\n\n\n\n\nsvd_reconstruct(grey_img, 30)\n\n\n\n\n\nsvd_reconstruct(grey_img, 50)\n\n\n\n\n\nsvd_reconstruct(grey_img, 100)\n\n\n\n\n\nsvd_reconstruct(grey_img, 200)\n\n\n\n\nAs shown in the above experiments, as the k increases, the reconstructed image became clearer and of higher quality, meanwhile, the percentage of storage space taken also increases as the k increases. When k is set at 30, the image is pretty much distinguishable from the original image by eye.\n\n\nConclusion\nThis blog post shows the process of image compression using singular value decomposition. By knocking off some entries in the matrix of the image, we get to reduce the storage space needed for this image. For the image of Copenhagen specifically, if we only take 30 components, the quality of the reconstructed image is okay (distinguishable), and when k = 200, the quality of the image is pretty much similar as the original greyscale image."
  },
  {
    "objectID": "posts/Blog_Post_Timnit_Gebru_Visit/Blog_Post_Learning_From_Timnit_Gebru.html",
    "href": "posts/Blog_Post_Timnit_Gebru_Visit/Blog_Post_Learning_From_Timnit_Gebru.html",
    "title": "Blog Post: Learning from Timnit Gebru",
    "section": "",
    "text": "What is Happening at Middlebury\nMiddlebury offers courses such as Artificial Intelligence and Machine Learning and regularly discusses topics related to ethics in AI.\nLike many other colleges, Middlebury is also facing the impact of ChatGPT. According to the Middlebury Campus Newspaper, the college has yet to establish a campus-wide policy regarding the use of ChatGPT or other AI tools, leaving the decision of whether to allow or prohibit such tools up to individual professors. However, there are ethical concerns associated with the use of ChatGPT and other AI tools. For instance, AI tools like ChatGPT and Grammarly have created a divide among students between those who can afford the premium version versus those who cannot, providing a significant advantage to students who can pay for premium features or information. Furthermore, large language models such as ChatGPT are trained on massive amounts of language data, which includes violent, racist, and sexist messages that people share online. Although developers try to mitigate such biased outputs before releasing ChatGPT, the chatbot still generates subtle and overt forms of bias.\n\n\nSummary of Dr. Timnit Gebru’s Talk\nIn her talk, Dr. Timnit Gebru discussed the current applications of computer vision technology and expressed her concerns about their unreliability and bias. She highlighted examples such as Faception and Hirevue, which use facial images to judge people’s personalities and emotions. Dr. Gebru emphasized that these measures cannot truly reflect a person’s true pattern and, to a great extent, perpetuate structural racism. Faception, in particular, singles out individuals from marginalized groups as terrorists.\nDr. Gebru also commented on the tendency of fields such as machine learning, computer vision, and data sciences to abstract things, which she believed was problematic since the application of AI is intended to deal with real people. She quoted Seeta Gangadharan’s statement that “papers tend to disappear people into mathematical equations,” highlighting the importance of considering the human aspect when designing and implementing AI.\nThe lack of diversity in datasets was another issue raised by Dr. Gebru. She cited her research on Gender Shades, which found that facial analysis datasets were predominantly composed of lighter-skinned individuals and males, leading to a high disparity in facial recognition results. Dr. Gebru also noted that while many people advocate for diversity in datasets, it is not enough to make things work equally well for everyone. Even if it does work for everyone, the situation can still be problematic. She discussed how some companies and institutions unethically gather “diverse” information by predatorily seeking more darker skin images and scraping images of transgender YouTubers without their consent.\nMoreover, Dr. Gebru stressed that technology is not always used in the way it is designed, and sometimes its use leads to more discrimination and problems than efficiency. People often rely too much on technology tools that ignore the real facts presented to them.\nDr. Gebru suggested that we need to think beyond diversity in datasets and consider structural and real representation. She highlighted that panels mainly comprise individuals from dominant groups and those closest to money, which can marginalize vulnerable groups. She concluded that fairness is not only about datasets or math but also about society, and computer scientists should not shy away from this fact.\n\n\nQuestions to Dr. Gebru\n\nConsidering the unreliable and biased nature of facial recognition tools, do you think it is necessary to prohibit or regulate the use of facial recognition on people, or to focus on improving the accuracy of the model and taking into account social factors?\nHow can computer vision technology assist marginalized communities, and what level of regulation do you believe is necessary?\n\n\n\nDr. Gebru’s Talk 4/24\nOn April 24th, Dr. Timnit Gebru was invited to give a talk on the topic of “Eugenics and the Promise of Utopia through Artificial General Intelligence”. She gave some interesting points and examples to discuss the current stage of AI development.\nDr. Gebru first claimed that the current AI system does not make the world a more equitable place. While founders of AI companies such as Sam Altman always talked about how AI could be a great force for economic empowerment and allow more people to become rich, the fact is that workers in underdeveloped regions, for example, Kenyan workers, are paid less than $2 per hour to make ChatGPT less toxic, meaning that they are exploited.Obviously, there is significant discrepancy between what the leaders are saying and what people are seeing happening.\nDr. Gebru mentioned that the general trend for AI industry seems to be moving towards Artificial General Intelligence (AGI), which OpenAI defines as “highly autonomous systems that outperform humans at most economically valuable work”, and the claims that many leaders in the AI industry made about AGI sounds like they are building a god that can do anything for people.\nThose thoughts on AGI have relations to the Second-Wave Eugenics starting from the 1990s. It involves notions about Transhumanism, Extropianism, Singularitarianism, Cosmism, Rationalism, Effective Altruism, and Longtermism, also known as the TESCREAL Bundle. Some major thoughts include “human will merge with technology”, “people can choose to radically enhance themselves and become posthuman with enhanced capabilities”, “colonization of space”, etc. When people are talking about being treated as human, living in the world, being given rights, we can see that the thoughts related to Eugenics and AGI is trying to something opposite by merging people with machines, making people “superintelligent”, and improving human stock.\nWhile AI company leaders are describing AGI as something that could create utopia for human society, Dr. Timnit Gebru threw a question: utopia for whom? There are problems such as perpetuating biases and promoting centralization of power regarding the development of AGI. Resources and investments are going to the big companies, such as OpenAI, Meta, Google, not to organizations that serve their own communities or a specific niche, such as Ghana NLP. There are benefits for those smaller companies that serve their own communities to exist, such as they know more about the community, so they can create better product that fits the community better, and the money, jobs, etc. would go to the communities, not big companies.\n\n\nReflection\nThrough this talk, Dr. Gebru demonstrated some of the problems or risks that people are facing regarding AI. I did not went to the talk in person because I was having my Math Midterm at that time, but it is very interesting to listen to the recording and think about her points. I felt that before this talk, I mainly consider biases as the negative side of AI, but Dr. Gebru went beyond the possible biases in models or data, and talked about the general trend of AI development. For myself, in order for my convenience, I would usually prefer some tools that can do a lot of things for me at once, which is somewhat similar to AGI, but listening to the talk made me think about how AGI might marginalize certain groups and lead to the situation where the big companies take all the profits. It also made me think of my career path: I was hoping to find a job in a big tech company, but will my work lead to the closing of small companies that serve marginalize communities?\nIn general, I think that Dr. Gebru was very skeptical about AGI, while I am not that skeptical. I think AGI would do a good job providing service to the dominant groups, and it could be beneficial as it has a lot of functions, so it is not a totally bad thing. We, as future developers, however, should be aware of the risks and problems associated with AGI, and try to minimize the negative effect of our moves."
  }
]