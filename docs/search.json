[
  {
    "objectID": "posts/Blog_Post_Perceptron/Blog_Post_Perceptron.html",
    "href": "posts/Blog_Post_Perceptron/Blog_Post_Perceptron.html",
    "title": "Blog Post: Perceptron",
    "section": "",
    "text": "Implementation of perceptron update\nPerceptron update of weight vector w follows this logic:\n\n\nCompute the dot product of w and X at index i. The sign of the dot product indicates the expected value of y.\n\nif dot product > 0, y is expected to be positive;\n\nif dot product < 0, y is expected to be negative;\nthe result can be computed using Perceptron.predict() function.\n\n\nLet actual_y * expected_y:\n\n\nif result >= 0, the expected value and actual value have the same sign, so the result is well-predicted, and there is no need to update w\n\nif result < 0, the expected value and actual value have different signs, so the result is not predicted well, therefore we need to update w to get a weight vector that can better predict the result\n\n\nAssign 0 to actual_y * expected_y >=0 and 1 to actual_y * expected_y < 0. The 0s and 1s can be considered as a determinant that helps to determine whether w will be updated:\n\n\nWhen determinant = 0, the value to update is 0, meaning no update is made;\n\nWhen determinant = 1, it means to update the w.\n\n\nFor every step in the updating process:\nWe add determinant * y_tilde (which is y * 2 - 1, and basically replace the 0 in y by -1) * the vector at index i of X_ to the original w to get an updated weight vector.\n\n\nThe function I used to update the weight vector is mainly Perceptron.fit(), while Perceptron.predict() is called inside Perceptron.fit() to help computing the determinant. Both Perceptron.predict() and the parts of Perceptron.fit() related to updating the weight vector w is below:\ndef predict2(self, X):\n    \"\"\"\n    Return a list of vector showing whether the dot product of w and X is greater than\n    -1: smaller than 0\n    1: greater than 0\n    \"\"\"\n\n    return np.sign(np.dot(X, self.w))\n\ndef fit(self, X, y, max_steps=1000):\n    \"\"\"\n    Update the weight vector w and the history vector which keeps track of the change of accuracy       \n    Parameters:\n    X: a matrix of predictor variables, with X.shape[0] observations and X.shape[1] features\n    y: a vector of binary labels 0 and 1\n    max_steps: number of loops going through, set default to 1000        \n    Return:\n    None\n    \"\"\"\n    \n    X_ = np.append(X, np.ones((X.shape[0], 1)), 1) # initialize X_ by appending 1 at the end of X            \n    \n    self.w = np.random.rand(X_.shape[1]) # initialize a random weight vector w \n    \n    for _ in range(max_steps):\n        i = np.random.randint(X_.shape[0]) # pick a random index i within the range of number of observations\n        y_tilde = 2*y-1 # transform y from a vector containing 0 and 1 to a vector containing -1 and 1\n        \n        determine = np.where((y_tilde[i]*self.predict2(X_[i])) < 0, 1, 0) \n        # determine = 0 if predicted and actual y have same sign\n        # determine = 1 if predicted and actual y have different sign, which triggers the update of w\n\n        self.w = self.w + determine*y_tilde[i]*X_[i] # update w\n\n\nExperiments\n\n1. 2D data in the example:\nVisualization of data:\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nfrom perceptron import Perceptron\n\np = Perceptron()\np.fit(X, y)\n\np.w\nprint(p.history[-10:])\n\n[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n\n\n\n\n\nEvolution of the score over training:\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nLine in the final iteration\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\n2. 2D data not linearly separable:\nVisualization of data:\n\nnp.random.seed(1234)\n\nn1 = 100\np1_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p1_features - 1, centers = [(-0.5, -0.5), (0.5, 0.5)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nfrom perceptron import Perceptron\n\np = Perceptron()\np.fit(X, y)\n\np.w\nprint(p.history[-10:])\n\n[0.48, 0.48, 0.48, 0.48, 0.48, 0.48, 0.48, 0.48, 0.42, 0.42]\n\n\n\n\n\nEvolution of the score over training:\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nLine in the final iteration:\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\n3. Data of more than two dimensions\n\nnp.random.seed(123)\n\nn2 = 100\np2_features = 5\n\nX, y = make_blobs(n_samples = 100, n_features = p2_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nfrom perceptron import Perceptron\n\np = Perceptron()\np.fit(X, y)\n\np.w\nprint(p.history[-10:])\n\n[0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96]\n\n\n\n\n\nEvolution of score over training period:\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nI believe that the data is not linearly separable. The max_steps I set was 1000, and after 1000 iterations, the accuracy stop increasing when it reaches the value of 0.96, indicating that it could draw a line that almost separate the data, but still not able to completely separate the data. Thus, the data is not linearly separable.\n\n\n\nRuntime Analysis\nThere is addition and multiplication involved in this operation.\nThe runtime complexity of a single iteration depends on the number of features p. This is because the weight vector w and the vector at index i of X which we compute dot product of, are all vector of p elements. If p increases, the runtime increases since more calculations need to be computed.\nThe runtime complexity of a single iteration does NOT depend on the number of data points n. Though y and predicted vector of y (denoted as y_predict) are vector of n elements, when we compute for updates, only the value at the index i of the y and y_predict will be used, so there is no looping through the whole list, thus n has no effect on the runtime.\nCalculating the dot product of w and X[i] should be O(n) since it loops through the whole list.\nMultiplying y_tilde[i] with the dot product would be O(1) since it is just the multiplication of two numbers.\nThe np.sign() operation here is also O(1) in 1 iteration since it only determines whether the result of previous two operations is larger or smaller than 0.\nMultiplying the result of previous three operations with y_tilde[i] and X[i] is has the complexity of O(n) since X[i] is a vector, and multiplication needs to loop through the whole vector.\nAddition of update vector to previous weight vector w has time complexity of O(n), since it needs to loop through the vector and add values at corresponding index.\nTherefore, the overall time complexity should be O(n)* O(n) + O(n) = O(n^2) + O(n) = O(n^2)"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "An example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog lol"
  },
  {
    "objectID": "posts/Blog_Post_Gradient_Descent/Blog_Post_Gradient_Descent.html",
    "href": "posts/Blog_Post_Gradient_Descent/Blog_Post_Gradient_Descent.html",
    "title": "Blog Post: Gradient Descent",
    "section": "",
    "text": "Implementation of Logistic Regression with Gradient Descent\n\nGradient descent update of weight vector w follows this formula:\n\\[w^{(t+1)} \\leftarrow w^{(t)} - \\alpha \\nabla L(w^{(t)})\\]\n\nCompute gradient of the empirical risk for logistic regression \\(\\nabla L(w)\\)\nAs shown in the class notes, \\[\\nabla L(w) = \\frac{1}{n} * \\sum \\limits _{i=1} ^{n} (\\sigma (<w, x_i>) - y_i)*x_i \\] We can calculate the gradient according to this formula, as illustrated in the gradient_descent function in the source code.\nFor every step in the updating process:\nWe subtract \\(\\alpha \\nabla L(w^{(t)})\\) from the current weight vector $ w^{(t)}$ to update the weight vector w until convergence.\n\n\n\nLoss history update follows this formula:\n\\[ l(\\hat{y}, y) = -ylog\\sigma(\\hat{y}) - (1-y)log(1-\\sigma(\\hat{y})) \\]\nwhere\n\\[ \\sigma (\\hat{y}) = \\frac{1}{1+e^{-\\hat{y}}} \\]\nWe just need to compute the average value of the loss function and append them to the loss_history list.\n\n\nStochastic gradient descent update:\nThe stochastic gradient descent follows the formula:\n\\[ \\nabla_S L(w) = \\frac{1}{|S|} \\sum \\limits_{i \\in S} \\nabla \\sigma (<w, x_i>) - y_i)*x_i \\]\nWe just need to compute the stochastic gradient descent and use it to update the weight vector as suggested by \\(w^{(t+1)} \\leftarrow w^{(t)} - \\alpha \\nabla L(w^{(t)})\\).\n\n\nMomentum (optional):\nMomentum is an optional variable \\(\\beta\\) that could possibly accelerate the program. According to p.85 of Hardt and Recht,\n\\[w_{k+1} = w_k - \\alpha_kg_k(w_k)+ \\beta(w_k - w_{k-1}) \\]\nTo implement momentum in fit_stochastic(), we need some variable that stores the previous \\(w\\): \\((w_{k-1})\\) and current \\(w\\): \\((w_k)\\) to compute the updated \\(w\\): (\\(w_{k+1}\\)).\n\nthe fit() function:\n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n# fit the model\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.1, max_epochs = 100)\n\n# inspect the fitted value of w\nprint(LR.w)\nprint(LR.loss_history[-10:])\nprint(LR.score_history[-10:])\n\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n# fit()\n\nloss = LR.loss(X, y)\n\nfig, axarr = plt.subplots(1, 2)\n\naxarr[0].scatter(X[:,0], X[:,1], c = y)\naxarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Loss = {loss}\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = axarr[0].plot(f1, (LR.w[2] - f1*LR.w[0])/LR.w[1], color = \"black\")\n\naxarr[1].plot(LR.loss_history)\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\nplt.tight_layout()\n\n[1.4488913  1.1878909  0.09130459]\n[0.20565486085080245, 0.20524657060908605, 0.20484531938583295, 0.204450929824252, 0.20406323050036534, 0.20368205567548014, 0.20330724506101988, 0.20293864359499791, 0.20257610122946323, 0.20221947272829266]\n[0.82, 0.82, 0.82, 0.82, 0.82, 0.82, 0.82, 0.83, 0.83, 0.83]\n\n\n\n\n\n\n\n\n\n\nthe fit_stochastic() function without momentum:\n\n# make the data for stochastic\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n# fit the model\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, alpha = 0.1, momentum = False, batch_size = 10, max_epochs = 100)\n\n# inspect the fitted value of w\n#LR.w \nprint(LR.loss_history[-10:])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n# fit_stochastic()\n\nloss = LR.loss(X, y)\n\nfig, axarr = plt.subplots(1, 2)\n\naxarr[0].scatter(X[:,0], X[:,1], c = y)\naxarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Loss = {loss}\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = axarr[0].plot(f1, (LR.w[2] - f1*LR.w[0])/LR.w[1], color = \"black\")\n\naxarr[1].plot(LR.loss_history)\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\nplt.tight_layout()\n\n[0.2259531715343903, 0.22595564897657913, 0.22595554249826233, 0.22595321577323918, 0.22595213303743808, 0.2259497420629399, 0.22595184551921593, 0.2259550169688368, 0.22596294630154048, 0.2259562448794485]\n\n\n\n\n\n\n\n\n\n\nfit_stochastic() function with momentum:\n\n# fit the model\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, alpha = 0.1, momentum = True, batch_size = 10, max_epochs = 100)\n\n# inspect the fitted value of w\n#LR.w \nprint(LR.loss_history[-10:])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n# fit_stochastic () with momentum\n\nloss = LR.loss(X, y)\n\nfig, axarr = plt.subplots(1, 2)\n\naxarr[0].scatter(X[:,0], X[:,1], c = y)\naxarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Loss = {loss}\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = axarr[0].plot(f1, (LR.w[2] - f1*LR.w[0])/LR.w[1], color = \"black\")\n\naxarr[1].plot(LR.loss_history)\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\nplt.tight_layout()\n\n[0.22597989694657847, 0.22618024352175845, 0.22629191258513107, 0.22608048805295794, 0.2260803900695497, 0.22604297177583932, 0.22599096301984894, 0.22617212604896203, 0.22607907289847376, 0.2259618004982282]\n\n\n\n\n\n\n\n\n\n\n\n\nExperiments\n\nfrom gradient_descent import LogisticRegression\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\n{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}\n\n\n\nCase 1: Gradient Descent does not converge to a minimizer because the learning rate \\(\\alpha\\) is too large\n\n# make the data\np_features = 10\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  momentum = True, \n                  batch_size = 10, \n                  alpha = .99) \n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient (momentum)\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = .99)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .99, max_epochs = 100)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\n\n\n\nWhen learning rate \\(\\alpha\\) is very large (close to 1), it actually converges. I think there might be something wrong in the description of experiment cases. I think that if learning rate is very small, it might not be able to converge to minimizer, as shown below:\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  momentum = True, \n                  batch_size = 10, \n                  alpha = .00001) \n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient (momentum)\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = .00001)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .00001, max_epochs = 100)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\n\n\n\nWe can see that the learning rate is too small that the gradient descent does not converge to a minimizer.\n\n\nCase 2: The choices of batch size influences how quickly the algorithm converges.\n\n# make the data\np_features = 10\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 1000, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = .05) \n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient with batch size = 10\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 1000, \n                  momentum = False, \n                  batch_size = 50, \n                  alpha = .05)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient with batch size = 50\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 1000, \n                  momentum = False, \n                  batch_size = 100, \n                  alpha = .05)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient with batch size = 100\")\n\n\nplt.loglog()\n\nlegend = plt.legend()\n\n\n\n\nIt seems that the function with smaller batch size converges faster.\n\n\nCase 3: Comparing the use of momentum to see if it speeds up the convergence\n\n# make the data\np_features = 10\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  momentum = True, \n                  batch_size = 10, \n                  alpha = .05) \n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient (momentum)\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = .05)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .05, max_epochs = 100)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\n\n\n\nAfter trying several times, it seems that function with momentum seems to converge faster than the function using normal stochastic gradient and normal empirical risk gradient."
  }
]