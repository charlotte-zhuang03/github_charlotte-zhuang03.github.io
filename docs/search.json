[
  {
    "objectID": "posts/Blog_Post_Perceptron/Blog_Post_Perceptron.html",
    "href": "posts/Blog_Post_Perceptron/Blog_Post_Perceptron.html",
    "title": "Blog Post: Perceptron",
    "section": "",
    "text": "Implementation of perceptron update\nPerceptron update of weight vector w follows this logic:\n\n\nCompute the dot product of w and X at index i. The sign of the dot product indicates the expected value of y.\n\nif dot product > 0, y is expected to be positive;\n\nif dot product < 0, y is expected to be negative;\nthe result can be computed using Perceptron.predict() function.\n\n\nLet actual_y * expected_y:\n\n\nif result >= 0, the expected value and actual value have the same sign, so the result is well-predicted, and there is no need to update w\n\nif result < 0, the expected value and actual value have different signs, so the result is not predicted well, therefore we need to update w to get a weight vector that can better predict the result\n\n\nAssign 0 to actual_y * expected_y >=0 and 1 to actual_y * expected_y < 0. The 0s and 1s can be considered as a determinant that helps to determine whether w will be updated:\n\n\nWhen determinant = 0, the value to update is 0, meaning no update is made;\n\nWhen determinant = 1, it means to update the w.\n\n\nFor every step in the updating process:\nWe add determinant * y_tilde (which is y * 2 - 1, and basically replace the 0 in y by -1) * the vector at index i of X_ to the original w to get an updated weight vector.\n\n\nThe function I used to update the weight vector is mainly Perceptron.fit(), while Perceptron.predict() is called inside Perceptron.fit() to help computing the determinant. Both Perceptron.predict() and the parts of Perceptron.fit() related to updating the weight vector w is below:\ndef predict2(self, X):\n    \"\"\"\n    Return a list of vector showing whether the dot product of w and X is greater than\n    -1: smaller than 0\n    1: greater than 0\n    \"\"\"\n\n    return np.sign(np.dot(X, self.w))\n\ndef fit(self, X, y, max_steps=1000):\n    \"\"\"\n    Update the weight vector w and the history vector which keeps track of the change of accuracy       \n    Parameters:\n    X: a matrix of predictor variables, with X.shape[0] observations and X.shape[1] features\n    y: a vector of binary labels 0 and 1\n    max_steps: number of loops going through, set default to 1000        \n    Return:\n    None\n    \"\"\"\n    \n    X_ = np.append(X, np.ones((X.shape[0], 1)), 1) # initialize X_ by appending 1 at the end of X            \n    \n    self.w = np.random.rand(X_.shape[1]) # initialize a random weight vector w \n    \n    for _ in range(max_steps):\n        i = np.random.randint(X_.shape[0]) # pick a random index i within the range of number of observations\n        y_tilde = 2*y-1 # transform y from a vector containing 0 and 1 to a vector containing -1 and 1\n        \n        determine = np.where((y_tilde[i]*self.predict2(X_[i])) < 0, 1, 0) \n        # determine = 0 if predicted and actual y have same sign\n        # determine = 1 if predicted and actual y have different sign, which triggers the update of w\n\n        self.w = self.w + determine*y_tilde[i]*X_[i] # update w\n\n\nExperiments\n\n1. 2D data in the example:\nVisualization of data:\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nfrom perceptron import Perceptron\n\np = Perceptron()\np.fit(X, y)\n\np.w\nprint(p.history[-10:])\n\n[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n\n\n\n\n\nEvolution of the score over training:\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nLine in the final iteration\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\n2. 2D data not linearly separable:\nVisualization of data:\n\nnp.random.seed(1234)\n\nn1 = 100\np1_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p1_features - 1, centers = [(-0.5, -0.5), (0.5, 0.5)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nfrom perceptron import Perceptron\n\np = Perceptron()\np.fit(X, y)\n\np.w\nprint(p.history[-10:])\n\n[0.48, 0.48, 0.48, 0.48, 0.48, 0.48, 0.48, 0.48, 0.42, 0.42]\n\n\n\n\n\nEvolution of the score over training:\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nLine in the final iteration:\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\n3. Data of more than two dimensions\n\nnp.random.seed(123)\n\nn2 = 100\np2_features = 5\n\nX, y = make_blobs(n_samples = 100, n_features = p2_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nfrom perceptron import Perceptron\n\np = Perceptron()\np.fit(X, y)\n\np.w\nprint(p.history[-10:])\n\n[0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96]\n\n\n\n\n\nEvolution of score over training period:\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nI believe that the data is not linearly separable. The max_steps I set was 1000, and after 1000 iterations, the accuracy stop increasing when it reaches the value of 0.96, indicating that it could draw a line that almost separate the data, but still not able to completely separate the data. Thus, the data is not linearly separable.\n\n\n\nRuntime Analysis\nThere is addition and multiplication involved in this operation.\nThe runtime complexity of a single iteration depends on the number of features p. This is because the weight vector w and the vector at index i of X which we compute dot product of, are all vector of p elements. If p increases, the runtime increases since more calculations need to be computed.\nThe runtime complexity of a single iteration does NOT depend on the number of data points n. Though y and predicted vector of y (denoted as y_predict) are vector of n elements, when we compute for updates, only the value at the index i of the y and y_predict will be used, so there is no looping through the whole list, thus n has no effect on the runtime.\nCalculating the dot product of w and X[i] should be O(n) since it loops through the whole list.\nMultiplying y_tilde[i] with the dot product would be O(1) since it is just the multiplication of two numbers.\nThe np.sign() operation here is also O(1) in 1 iteration since it only determines whether the result of previous two operations is larger or smaller than 0.\nMultiplying the result of previous three operations with y_tilde[i] and X[i] is has the complexity of O(n) since X[i] is a vector, and multiplication needs to loop through the whole vector.\nAddition of update vector to previous weight vector w has time complexity of O(n), since it needs to loop through the vector and add values at corresponding index.\nTherefore, the overall time complexity should be O(n)* O(n) + O(n) = O(n^2) + O(n) = O(n^2)"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "An example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog lol"
  }
]